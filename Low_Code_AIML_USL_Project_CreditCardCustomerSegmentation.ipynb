{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "Q1WSYF2n2rym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKkaX7G8vthR"
      },
      "source": [
        "### Context\n",
        "\n",
        "\n",
        "AllLife Bank wants to focus on its credit card customer base in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the Marketing team proposes to run personalized campaigns to target new customers as well as upsell to existing customers. Another insight from the market research was that the customers perceive the support services of the back poorly. Based on this, the Operations team wants to upgrade the service delivery model, to ensure that customer queries are resolved faster. Head of Marketing and Head of Delivery both decide to reach out to the Data Science team for help\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "To identify different segments in the existing customer, based on their spending patterns as well as past interaction with the bank, using clustering algorithms, and provide recommendations to the bank on how to better market to and service these customers.\n",
        "\n",
        "\n",
        "###  Data Description\n",
        "\n",
        "The data provided is of various customers of a bank and their financial attributes like credit limit, the total number of credit cards the customer has, and different channels through which customers have contacted the bank for any queries (including visiting the bank, online and through a call center).\n",
        "\n",
        "**Data Dictionary**\n",
        "\n",
        "- Sl_No: Primary key of the records\n",
        "- Customer Key: Customer identification number\n",
        "- Average Credit Limit: Average credit limit of each customer for all credit cards\n",
        "- Total credit cards: Total number of credit cards possessed by the customer\n",
        "- Total visits bank: Total number of Visits that customer made (yearly) personally to the bank\n",
        "- Total visits online: Total number of visits or online logins made by the customer (yearly)\n",
        "- Total calls made: Total number of calls made by the customer to the bank or its customer service department (yearly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHOIdlwcrqR"
      },
      "source": [
        "### **Please read the instructions carefully before starting the project.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. \n",
        "* Blanks '_______' are provided in the notebook that \n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space. \n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n"
      ],
      "metadata": {
        "id": "_Z5w4qCJtZOg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLsScK1DxrHX"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTBQbhKxvthT"
      },
      "outputs": [],
      "source": [
        "# this will help in making the Python code more structured automatically (good coding practice)\n",
        "# %load_ext nb_black\n",
        "\n",
        "# Libraries to help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "# to scale the data using z-score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to compute distances\n",
        "from scipy.spatial.distance import cdist, pdist\n",
        "\n",
        "# to perform k-means clustering and compute silhouette scores\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# to visualize the elbow curve and silhouette scores\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "\n",
        "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "\n",
        "# to suppress warnings\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "Hm7KgGlU3IIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz5HNzkNvthX"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('____')## Complete the code to import the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of the Dataset"
      ],
      "metadata": {
        "id": "EmcUelfD3gP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial steps to get an overview of any dataset is to: \n",
        "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- get information about the number of rows and columns in the dataset\n",
        "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
        "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
      ],
      "metadata": {
        "id": "_fhnpafoI1GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the shape of the dataset"
      ],
      "metadata": {
        "id": "hwQCJyE03W61"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dMsbWyo-vthX"
      },
      "outputs": [],
      "source": [
        "# checking shape of the data\n",
        "print(f\"There are {'______'} rows and {'______'} columns.\") ## Complete the code to get the shape of data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying few rows of the dataset"
      ],
      "metadata": {
        "id": "v85RmCY83qfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kQ9LRK-DvthY"
      },
      "outputs": [],
      "source": [
        "# viewing the first 5 rows of the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a copy of original data"
      ],
      "metadata": {
        "id": "Bzr4r3ua3usT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copying the data to another variable to avoid any changes to original data\n",
        "df = data.copy()"
      ],
      "metadata": {
        "id": "_BL-FDlvtoWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing column names\n",
        "df.columns = [c.replace(\" \", \"_\") for c in df.columns]"
      ],
      "metadata": {
        "id": "YfmELFEItoNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the data types of the columns for the dataset"
      ],
      "metadata": {
        "id": "D6y0mZgY3zA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBLkLZXVvtha"
      },
      "outputs": [],
      "source": [
        "# checking datatypes and number of non-null values for each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgr0jhQjvtha"
      },
      "source": [
        "- All the columns in the data are numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the missing values "
      ],
      "metadata": {
        "id": "cdqyKS-6woGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXR-o4R8vthb"
      },
      "outputs": [],
      "source": [
        "# checking for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwmUu5cRvthb"
      },
      "source": [
        "- There are no missing values in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn7JqS8Tvthb"
      },
      "outputs": [],
      "source": [
        "# checking the number of unique values in each column\n",
        "data.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for duplicates values"
      ],
      "metadata": {
        "id": "7qcQovqA31ZM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oULmE0Ovthc"
      },
      "source": [
        "**Let's look at the duplicate values in the *Customer_Key* column closely.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "kwS_I_5nvthc"
      },
      "outputs": [],
      "source": [
        "# getting the count for each unique value in Customer_Key\n",
        "data_grouped = df.groupby(\"Customer_Key\").count()\n",
        "\n",
        "for i in data_grouped.loc[data_grouped.Sl_No >= 2].index:\n",
        "    display(data.loc[df.Customer_Key == i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y1aok7rvthd"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[\"____\"], inplace=True) # complete the code to drop the columns \n",
        "df.drop(columns=[\"____\"], inplace=True) # complete the code to drop the columns "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical summary of the dataset"
      ],
      "metadata": {
        "id": "sSju8Xrl4HZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgo41zvgvthd"
      },
      "outputs": [],
      "source": [
        "# Let's look at the statistical summary of the data\n",
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPuzWO7hmV8"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YyWJgFlKlWM"
      },
      "source": [
        "#### The below functions need to be defined to carry out the Exploratory Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22FHG_Vuvthe"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTYgB8kKvthf"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPC-HYczvthe"
      },
      "source": [
        "### Univariate analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's see the distribution of different variables in the dataset**"
      ],
      "metadata": {
        "id": "0e1SY_daOA2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "o3nPTIx6vthe"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    histogram_boxplot(data, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgyC9U9jvthf"
      },
      "outputs": [],
      "source": [
        "for col in df.columns.tolist()[1:]:\n",
        "    labeled_barplot(df, col, perc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7G1qBSvthg"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
        "fig.suptitle(\"CDF plot of numerical variables\", fontsize=20)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for ii in range(3):\n",
        "    sns.ecdfplot(data=df, ax=axes[ii][0], x=df.columns.tolist()[counter])\n",
        "    counter = counter + 1\n",
        "    if counter != 5:\n",
        "        sns.ecdfplot(data=df, ax=axes[ii][1], x=df.columns.tolist()[counter])\n",
        "        counter = counter + 1\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "fig.tight_layout(pad=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFZa4IBWvthh"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N27jPMDLvthh"
      },
      "source": [
        "**Let's check for correlations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVXMCBTUvthh"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RigRGSE8vthh"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=df, diag_kind=\"kde\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "WmBblONRvthi"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(\n",
        "    data=df[\n",
        "        [\n",
        "            \"Total_visits_bank\",\n",
        "            \"Total_visits_online\",\n",
        "            \"Total_calls_made\",\n",
        "            \"Total_Credit_Cards\",\n",
        "        ]\n",
        "    ],\n",
        "    hue=\"Total_Credit_Cards\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "zWuO2vMSvthi"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = plt.axes(projection=\"3d\")\n",
        "\n",
        "x = df[\"Total_visits_bank\"]\n",
        "y = df[\"Total_visits_online\"]\n",
        "z = df[\"Total_calls_made\"]\n",
        "\n",
        "\n",
        "ax.scatter(x, y, z, marker=\".\")\n",
        "ax.set_xlabel(\"Total_visits_bank\")\n",
        "ax.set_ylabel(\"Total_visits_online\")\n",
        "ax.set_zlabel(\"Total_calls_made\")\n",
        "ax.view_init(azim=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRe1tDXnvthj"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2d1KhBBvthj"
      },
      "source": [
        "### Outlier Detection\n",
        "\n",
        "- Let's find outliers in the data using z-score with a threshold of 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMYxa73Ovthj"
      },
      "outputs": [],
      "source": [
        "threshold = ____ # wite the value of the threshold \n",
        "outlier = {}\n",
        "for col in df.columns:\n",
        "    i = df[col]\n",
        "    mean = np.mean(df[col])\n",
        "    std = np.std(df[col])\n",
        "    list1 = []\n",
        "    for v in i:\n",
        "        z = (v - mean) / std\n",
        "        if z > threshold:\n",
        "            list1.append(v)\n",
        "    list1.sort()\n",
        "    outlier[i.name] = list1\n",
        "\n",
        "print(\"The following are the outliers in the data:\")\n",
        "for key, value in outlier.items():\n",
        "    print(\"\\n\", key, \":\", value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW5GD4lGvthk"
      },
      "source": [
        "### Scaling\n",
        "\n",
        "- Let's scale the data before we proceed with clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO2hJPDMvthk"
      },
      "outputs": [],
      "source": [
        "# scaling the data before clustering\n",
        "scaler = StandardScaler()\n",
        "subset = \"___\"  ## Complete the code to scale the data\n",
        "subset_scaled = scaler.fit_transform(subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC7fJ53dvthk"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe of the scaled data\n",
        "subset_scaled_df = pd.DataFrame(subset_scaled, columns=subset.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VrXo1JGxrIB"
      },
      "source": [
        "## K-means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Elbow Plot"
      ],
      "metadata": {
        "id": "8qinndqpJFcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HZ1T21Xvthl"
      },
      "outputs": [],
      "source": [
        "k_means_df = subset_scaled_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGc0pwexvthl"
      },
      "outputs": [],
      "source": [
        "clusters = range(2, 11)\n",
        "meanDistortions = []\n",
        "\n",
        "for k in clusters:\n",
        "    model = KMeans(n_clusters=k, random_state=1)\n",
        "    model.fit(subset_scaled_df)\n",
        "    prediction = model.predict(k_means_df)\n",
        "    distortion = sum(\n",
        "        np.min(cdist(k_means_df, model.cluster_centers_, \"euclidean\"), axis=1) ** 2)\n",
        "\n",
        "    meanDistortions.append(distortion)\n",
        "\n",
        "    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)\n",
        "\n",
        "plt.plot(clusters, meanDistortions, \"bx-\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Average Distortion\")\n",
        "plt.title(\"Selecting k with the Elbow Method\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JTiXD3rvthl"
      },
      "outputs": [],
      "source": [
        "model = KMeans(random_state=1)\n",
        "visualizer = KElbowVisualizer(model, k=(2, 10), timings=True)\n",
        "visualizer.fit(k_means_df)  # fit the data to the visualizer\n",
        "visualizer.show()  # finalize and render figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VIgju4dxrIC"
      },
      "source": [
        "### Let's check the silhouette scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuPeCdLNvthm"
      },
      "outputs": [],
      "source": [
        "sil_score = []\n",
        "cluster_list = range(2, 10)\n",
        "for n_clusters in cluster_list:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    preds = clusterer.fit_predict((subset_scaled_df))\n",
        "    score = silhouette_score(k_means_df, preds)\n",
        "    sil_score.append(score)\n",
        "    print(\"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QWIUkERVvthm"
      },
      "outputs": [],
      "source": [
        "plt.plot(cluster_list, sil_score)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's visualize the silhouette scores for different number of clusters**"
      ],
      "metadata": {
        "id": "S4Ryh_pVuY2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjJ0Jwayvthm"
      },
      "outputs": [],
      "source": [
        "model = KMeans(random_state=1)\n",
        "visualizer = KElbowVisualizer(model, k=(____, ___), metric=\"silhouette\", timings=True) ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
        "visualizer.fit(k_means_df)  # fit the data to the visualizer\n",
        "visualizer.show()  # finalize and render figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb6MgDZyvthn"
      },
      "outputs": [],
      "source": [
        "# finding optimal no. of clusters with silhouette coefficients\n",
        "visualizer = SilhouetteVisualizer(KMeans('___', random_state=1))  ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
        "visualizer.fit(k_means_df)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69p_leZqvthn"
      },
      "outputs": [],
      "source": [
        "# finding optimal no. of clusters with silhouette coefficients\n",
        "visualizer = SilhouetteVisualizer(KMeans('___', random_state=1))  ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
        "visualizer.fit(k_means_df)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWacdG16vthn"
      },
      "outputs": [],
      "source": [
        "# finding optimal no. of clusters with silhouette coefficients\n",
        "visualizer = SilhouetteVisualizer(KMeans('___', random_state=1))  ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
        "visualizer.fit(k_means_df)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NJOblDtfvtho"
      },
      "outputs": [],
      "source": [
        "# finding optimal no. of clusters with silhouette coefficients\n",
        "visualizer = SilhouetteVisualizer(KMeans('___', random_state=1))  ## Complete the code to visualize the silhouette scores for certain number of clusters\n",
        "visualizer.fit(k_means_df)\n",
        "visualizer.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Final Model"
      ],
      "metadata": {
        "id": "NZ1Ec654JPGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGrZKNuTvtho"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "kmeans = KMeans(n_clusters='___', random_state=1)  ## Complete the code to choose the number of clusters\n",
        "kmeans.fit(k_means_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us9LbPaAvtho"
      },
      "outputs": [],
      "source": [
        "# creating a copy of the original data\n",
        "df1 = df.copy()\n",
        "\n",
        "# adding kmeans cluster labels to the original and scaled dataframes\n",
        "k_means_df[\"K_means_segments\"] = kmeans.labels_\n",
        "df1[\"K_means_segments\"] = kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcr07xnXxrIH"
      },
      "source": [
        "## Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Cophenetic Correlation"
      ],
      "metadata": {
        "id": "lyZNLbb24zO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBPPNSKpvthp"
      },
      "outputs": [],
      "source": [
        "hc_df = subset_scaled_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m0E2X-Zvthp"
      },
      "outputs": [],
      "source": [
        "# list of distance metrics\n",
        "distance_metrics = ['___'] ## Complete the code to add distance metrics\n",
        "\n",
        "# list of linkage methods\n",
        "linkage_methods = ['___'] ## Complete the code to add linkages\n",
        "\n",
        "high_cophenet_corr = 0\n",
        "high_dm_lm = [0, 0]\n",
        "\n",
        "for dm in distance_metrics:\n",
        "    for lm in linkage_methods:\n",
        "        Z = linkage(hc_df, metric=dm, method=lm)\n",
        "        c, coph_dists = cophenet(Z, pdist(hc_df))\n",
        "        print(\n",
        "            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n",
        "                dm.capitalize(), lm, c\n",
        "            )\n",
        "        )\n",
        "        if high_cophenet_corr < c:\n",
        "            high_cophenet_corr = c\n",
        "            high_dm_lm[0] = dm\n",
        "            high_dm_lm[1] = lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRkoMtnCvthp"
      },
      "outputs": [],
      "source": [
        "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
        "print(\n",
        "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.\".format(\n",
        "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAGBKFHZvthp"
      },
      "source": [
        "**Let's explore different linkage methods with Euclidean distance only.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj-ucPg0vthq"
      },
      "outputs": [],
      "source": [
        "# list of linkage methods\n",
        "linkage_methods = ['___'] ## Complete the code to add linkages\n",
        "\n",
        "high_cophenet_corr = 0\n",
        "high_dm_lm = [0, 0]\n",
        "\n",
        "for lm in linkage_methods:\n",
        "    Z = linkage(hc_df, metric=\"euclidean\", method=lm)\n",
        "    c, coph_dists = cophenet(Z, pdist(hc_df))\n",
        "    print(\"Cophenetic correlation for {} linkage is {}.\".format(lm, c))\n",
        "    if high_cophenet_corr < c:\n",
        "        high_cophenet_corr = c\n",
        "        high_dm_lm[0] = \"euclidean\"\n",
        "        high_dm_lm[1] = lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt_xoqo6vthq"
      },
      "outputs": [],
      "source": [
        "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
        "print()\n",
        "print(\n",
        "    \"Highest cophenetic correlation is {}, which is obtained with {} linkage.\".format(\n",
        "        high_cophenet_corr, high_dm_lm[1]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Dendrograms"
      ],
      "metadata": {
        "id": "WfGygoZp47c-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEoa_DPcvthq"
      },
      "source": [
        "**We see that the cophenetic correlation is maximum with Euclidean distance and average linkage.**\n",
        "\n",
        "\n",
        "**Let's view the dendrograms for the different linkage methods.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z4uGLLnvthq"
      },
      "outputs": [],
      "source": [
        "# list of linkage methods\n",
        "linkage_methods = ['___'] ## Complete the code to add linkages\n",
        "\n",
        "# lists to save results of cophenetic correlation calculation\n",
        "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
        "compare = []\n",
        "\n",
        "# to create a subplot image\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n",
        "\n",
        "# We will enumerate through the list of linkage methods above\n",
        "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    Z = linkage(hc_df, metric=\"euclidean\", method=method)\n",
        "\n",
        "    dendrogram(Z, ax=axs[i])\n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
        "\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(hc_df))\n",
        "    axs[i].annotate(\n",
        "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
        "        (0.80, 0.80),\n",
        "        xycoords=\"axes fraction\",\n",
        "    )\n",
        "\n",
        "    compare.append([method, coph_corr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT8qJk6fvthr"
      },
      "source": [
        "**Dendrogram with average linkage shows distinct and separate cluster tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7R8US7Cvthr"
      },
      "outputs": [],
      "source": [
        "# create and print a dataframe to compare cophenetic correlations for different linkage methods\n",
        "df_cc = pd.DataFrame(compare, columns=compare_cols)\n",
        "df_cc = df_cc.sort_values(by=\"Cophenetic Coefficient\")\n",
        "df_cc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Final Model"
      ],
      "metadata": {
        "id": "5UB39umrJO04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INd-BH1hvths"
      },
      "outputs": [],
      "source": [
        "HCmodel = AgglomerativeClustering(n_clusters='___', affinity='___', linkage='___')  ## Complete the code to define the hierarchical clustering model\n",
        "HCmodel.fit(hc_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mLjHWISvths"
      },
      "outputs": [],
      "source": [
        "# creating a copy of the original data\n",
        "df2 = df.copy()\n",
        "\n",
        "# adding hierarchical cluster labels to the original and scaled dataframes\n",
        "hc_df[\"HC_segments\"] = HCmodel.labels_\n",
        "df2[\"HC_segments\"] = HCmodel.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuTMIWdLvths"
      },
      "outputs": [],
      "source": [
        "hc_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YefJ_zWxvths"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEPz_WSwvtht"
      },
      "outputs": [],
      "source": [
        "subset_scaled_df[\"HC_Clusters\"] = HCmodel.labels_\n",
        "df[\"HC_Clusters\"] = HCmodel.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-jOhqZqvtht"
      },
      "source": [
        "## Cluster Profiling and Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdugO0Z6vtht"
      },
      "source": [
        "### Cluster Profiling: K-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnNwbB6Cvtht"
      },
      "outputs": [],
      "source": [
        "km_cluster_profile = df1.groupby(\"___\").mean()  ## Complete the code to groupby the cluster labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7pzhKsDvtht"
      },
      "outputs": [],
      "source": [
        "km_cluster_profile[\"count_in_each_segment\"] = (\n",
        "    df1.groupby(\"___\")[\"Avg_Credit_Limit\"].count().values  ## Complete the code to groupby the cluster labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHydx11nvthu"
      },
      "outputs": [],
      "source": [
        "km_cluster_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwoUuWgQvthu"
      },
      "source": [
        "### Cluster Profiling: Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK7E-Xnrvthu"
      },
      "outputs": [],
      "source": [
        "hc_cluster_profile = df2.groupby(\"HC_segments\").mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-e8lYyZvthu"
      },
      "outputs": [],
      "source": [
        "hc_cluster_profile[\"count_in_each_segment\"] = (\n",
        "    df2.groupby(\"HC_segments\")[\"Avg_Credit_Limit\"].count().values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHbBXcbPvthv"
      },
      "outputs": [],
      "source": [
        "hc_cluster_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F21MiwFxrIO"
      },
      "source": [
        "## K-means vs Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOYnz3Huxov5"
      },
      "source": [
        "You compare several things, like:\n",
        "- Which clustering technique took less time for execution?\n",
        "- Which clustering technique gave you more distinct clusters, or are they the same?\n",
        "- How many observations are there in the similar clusters of both algorithms?\n",
        "- How many clusters are obtained as the appropriate number of clusters from both algorithms?\n",
        "\n",
        "You can also mention any differences or similarities you obtained in the cluster profiles from both the clustering techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V1HRdm39vthv"
      },
      "outputs": [],
      "source": [
        "km_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "p2yu4tKlvthv"
      },
      "outputs": [],
      "source": [
        "hc_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_zxH8Pavthv"
      },
      "outputs": [],
      "source": [
        "k_means_df.groupby(\"K_means_segments\").mean().plot.bar(figsize=(15, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJo0M78_vthv"
      },
      "outputs": [],
      "source": [
        "hc_df.groupby(\"HC_segments\").mean().plot.bar(figsize=(15, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "f_ZNzQ1-vthw"
      },
      "outputs": [],
      "source": [
        "k_means_df.loc[k_means_df[\"K_means_segments\"] == 1, \"K_means_segments\"] = 3\n",
        "k_means_df.loc[k_means_df[\"K_means_segments\"] == 2, \"K_means_segments\"] = 1\n",
        "k_means_df.loc[k_means_df[\"K_means_segments\"] == 3, \"K_means_segments\"] = 2\n",
        "df1[\"K_means_segments\"] = k_means_df[\"K_means_segments\"]\n",
        "\n",
        "km_cluster_profile = df1.groupby(\"K_means_segments\").mean()\n",
        "km_cluster_profile[\"count_in_each_segment\"] = (\n",
        "    df1.groupby(\"K_means_segments\")[\"Avg_Credit_Limit\"].count().values\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zjs2vp0vthw"
      },
      "outputs": [],
      "source": [
        "km_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbHxs6qRvthw"
      },
      "outputs": [],
      "source": [
        "k_means_df.groupby(\"K_means_segments\").mean().plot.bar(figsize=(15, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJ_R_DJvthx"
      },
      "source": [
        "**Let's create some plots on the original data to understand the customer distribution among the clusters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwCU3GpNvthx"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(16, 6))\n",
        "fig.suptitle(\n",
        "    \"Boxplot of numerical variables for each cluster obtained using K-means Clustering\",\n",
        "    fontsize=20,\n",
        ")\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for ii in range(5):\n",
        "    sns.boxplot(\n",
        "        ax=axes[ii], y=df1[df1.columns[counter]], x=k_means_df[\"K_means_segments\"]\n",
        "    )\n",
        "    counter = counter + 1\n",
        "\n",
        "fig.tight_layout(pad=2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQzbFG8ivthx"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(16, 6))\n",
        "fig.suptitle(\n",
        "    \"Boxplot of numerical variables for each cluster obtained using Hierarchical Clustering\",\n",
        "    fontsize=20,\n",
        ")\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for ii in range(5):\n",
        "    sns.boxplot(ax=axes[ii], y=df2[df2.columns[counter]], x=hc_df[\"HC_segments\"])\n",
        "    counter = counter + 1\n",
        "\n",
        "fig.tight_layout(pad=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actionable Insights and Recommendations"
      ],
      "metadata": {
        "id": "OKrYpEMZ4zKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \n"
      ],
      "metadata": {
        "id": "1XdRy-QU3xV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "DbyjZdcE3yDc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mLsScK1DxrHX",
        "Hm7KgGlU3IIZ",
        "hwQCJyE03W61",
        "v85RmCY83qfc",
        "Bzr4r3ua3usT",
        "D6y0mZgY3zA8",
        "cdqyKS-6woGg",
        "7qcQovqA31ZM",
        "sSju8Xrl4HZk",
        "-YyWJgFlKlWM",
        "aPC-HYczvthe",
        "mFZa4IBWvthh",
        "A2d1KhBBvthj",
        "eW5GD4lGvthk",
        "8qinndqpJFcQ",
        "OKrYpEMZ4zKq"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}